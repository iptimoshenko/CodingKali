{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iptimoshenko/CodingKali/blob/master/PDF_Query_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlorSbccWEDa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ac7692f-962b-4458-9e5c-39ea015aa0a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.340-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.2-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
            "  Downloading langsmith-0.0.66-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.2 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.340 langsmith-0.0.66 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting openai\n",
            "  Downloading openai-1.3.5-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.8/220.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.5\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "!pip install faiss-cpu\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS"
      ],
      "metadata": {
        "id": "nq0vKGFeW1KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get your API keys from openai, you will need to create an account.\n",
        "# Here is the link to get the keys: https://platform.openai.com/account/billing/overview\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "yKaKB_GjWKjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connect your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuSRy_lbWfE3",
        "outputId": "831d5658-6ec5-4ec6-fdea-ca6fdfbcfded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# location of the pdf file/files.\n",
        "reader = PdfReader(os.path.join(root_dir, 'ecstatic_earthling/apps/The-Field-Guide-to-Data-Science.pdf'))"
      ],
      "metadata": {
        "id": "NalD3XkQWrJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwbEBhd0ZUfX",
        "outputId": "c542e392-bad8-4938-d77b-501b981e6d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PyPDF2._reader.PdfReader at 0x7f7538f02340>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read data from the file and put them into a variable called raw_text\n",
        "raw_text = ''\n",
        "for i, page in enumerate(reader.pages):\n",
        "    text = page.extract_text()\n",
        "    if text:\n",
        "        raw_text += text"
      ],
      "metadata": {
        "id": "2VXlucKiW7bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(raw_text)"
      ],
      "metadata": {
        "id": "Gy3UwHGAZa0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "557970d4-3ce6-451d-a601-dad6c431a843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "162337"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CQkqUBlzW-Xv",
        "outputId": "0b472f1a-9314-4ef0-90a3-27017a84a629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'    THE FIELD  GUIDE   \\n            to   DATA  SCIENCE\\n© COPYRIGHT 2013 BOOZ ALLEN HAMILTON INC. ALL'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to split the text that we read into smaller chunks so that during information retreival we don't hit the token size limits.\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap  = 200,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "VdXzkpf9XAfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozkNTiNuZ0TX",
        "outputId": "b8d65ac9-2b0a-4a9c-ad15-f73b0738c4f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "202"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "1SqdR3wFZ3Ih",
        "outputId": "6703c979-e153-46d5-c9c7-617ec872f00c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'THE FIELD  GUIDE   \\n            to   DATA  SCIENCE\\n© COPYRIGHT 2013 BOOZ ALLEN HAMILTON INC. ALL RIGHTS RESERVED.FOREWORD\\nEvery aspect of our lives, from life-saving disease \\ntreatments, to national security, to economic stability \\nand even the convenience of selecting a restaurant, \\ncan be improved by creating better data analytics \\nthrough Data Science. \\nWe live in a world of incredible \\nbeauty and complexity. A world \\nincreasingly measured, mapped, \\nand recorded into digital bits for \\neternity. Our human existence \\nis pouring into the digital realm \\nfaster than ever. From global \\nbusiness operations to simple \\nexpressions of love – an essential \\npart of humanity now exists in \\nthe digital world. \\nData  is the byproduct of our \\nnew digital existence.  Recorded \\nbits of data from mundane \\ntraﬃc cameras to telescopes \\npeering into the depths of \\nspace are propelling us into \\nthe greatest age of discovery \\nour species has ever known. \\nAs we move from isolation into'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "059PoKYUZ6dJ",
        "outputId": "7abb2618-de8c-4a01-f190-f3f6370a2cac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'traﬃc cameras to telescopes \\npeering into the depths of \\nspace are propelling us into \\nthe greatest age of discovery \\nour species has ever known. \\nAs we move from isolation into \\nour ever-connected and recorded \\nfuture, data is becoming the \\nnew currency and a vital natural \\nresource. /T_he power, importance, and responsibility such incredible \\ndata stewardship will demand of \\nus in the coming decades is hard \\nto imagine – but we often fail to \\nfully appreciate the insights data \\ncan provide us today. Businesses \\nthat do not rise to the occasion \\nand garner insights from this new \\nresource are destined for failure.\\nAn essential part of human \\nnature is our insatiable curiosity \\nand the need to /f_ind answers to \\nour hardest problems. Today, the \\nemerging /f_ield of Data Science is \\nan auspicious and profound new \\nway of applying our curiosity \\nand technical tradecraft to create \\nvalue from data that solves our \\nhardest problems. Leaps in \\nhuman imagination, vast amounts'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download embeddings from OpenAI\n",
        "embeddings = OpenAIEmbeddings(disallowed_special=())"
      ],
      "metadata": {
        "id": "TcZUsQVyXBPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MULxz4gPPQ8",
        "outputId": "5f5121fe-2e49-431e-ed18-6fa056599afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7f2519108df0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7f251912ed70>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization=None, allowed_special=set(), disallowed_special=set(), chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, http_client=None)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ"
      ],
      "metadata": {
        "id": "OzjB3MZZUEaz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a8edac5-da79-4098-e0be-3ebc58d7ace7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "environ{'SHELL': '/bin/bash',\n",
              "        'NV_LIBCUBLAS_VERSION': '11.11.3.6-1',\n",
              "        'NVIDIA_VISIBLE_DEVICES': 'all',\n",
              "        'COLAB_JUPYTER_TRANSPORT': 'ipc',\n",
              "        'NV_NVML_DEV_VERSION': '11.8.86-1',\n",
              "        'NV_CUDNN_PACKAGE_NAME': 'libcudnn8',\n",
              "        'CGROUP_MEMORY_EVENTS': '/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events',\n",
              "        'NV_LIBNCCL_DEV_PACKAGE': 'libnccl-dev=2.15.5-1+cuda11.8',\n",
              "        'NV_LIBNCCL_DEV_PACKAGE_VERSION': '2.15.5-1',\n",
              "        'VM_GCE_METADATA_HOST': '169.254.169.253',\n",
              "        'HOSTNAME': '13b3e23f22f6',\n",
              "        'LANGUAGE': 'en_US',\n",
              "        'TBE_RUNTIME_ADDR': '172.28.0.1:8011',\n",
              "        'GCE_METADATA_TIMEOUT': '3',\n",
              "        'NVIDIA_REQUIRE_CUDA': 'cuda>=11.8 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471',\n",
              "        'NV_LIBCUBLAS_DEV_PACKAGE': 'libcublas-dev-11-8=11.11.3.6-1',\n",
              "        'NV_NVTX_VERSION': '11.8.86-1',\n",
              "        'COLAB_JUPYTER_IP': '172.28.0.12',\n",
              "        'NV_CUDA_CUDART_DEV_VERSION': '11.8.89-1',\n",
              "        'NV_LIBCUSPARSE_VERSION': '11.7.5.86-1',\n",
              "        'COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL': 'http://172.28.0.1:8013/',\n",
              "        'NV_LIBNPP_VERSION': '11.8.0.86-1',\n",
              "        'NCCL_VERSION': '2.15.5-1',\n",
              "        'KMP_LISTEN_PORT': '6000',\n",
              "        'TF_FORCE_GPU_ALLOW_GROWTH': 'true',\n",
              "        'ENV': '/root/.bashrc',\n",
              "        'PWD': '/',\n",
              "        'TBE_EPHEM_CREDS_ADDR': '172.28.0.1:8009',\n",
              "        'COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT': '30s',\n",
              "        'TBE_CREDS_ADDR': '172.28.0.1:8008',\n",
              "        'NV_CUDNN_PACKAGE': 'libcudnn8=8.9.6.50-1+cuda11.8',\n",
              "        'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility',\n",
              "        'COLAB_JUPYTER_TOKEN': '',\n",
              "        'LAST_FORCED_REBUILD': '20231120',\n",
              "        'NV_NVPROF_DEV_PACKAGE': 'cuda-nvprof-11-8=11.8.87-1',\n",
              "        'NV_LIBNPP_PACKAGE': 'libnpp-11-8=11.8.0.86-1',\n",
              "        'NV_LIBNCCL_DEV_PACKAGE_NAME': 'libnccl-dev',\n",
              "        'TCLLIBPATH': '/usr/share/tcltk/tcllib1.20',\n",
              "        'NV_LIBCUBLAS_DEV_VERSION': '11.11.3.6-1',\n",
              "        'NVIDIA_PRODUCT_NAME': 'CUDA',\n",
              "        'COLAB_KERNEL_MANAGER_PROXY_HOST': '172.28.0.12',\n",
              "        'NV_LIBCUBLAS_DEV_PACKAGE_NAME': 'libcublas-dev-11-8',\n",
              "        'NV_CUDA_CUDART_VERSION': '11.8.89-1',\n",
              "        'COLAB_WARMUP_DEFAULTS': '1',\n",
              "        'HOME': '/root',\n",
              "        'LANG': 'en_US.UTF-8',\n",
              "        'COLUMNS': '100',\n",
              "        'CUDA_VERSION': '11.8.0',\n",
              "        'CLOUDSDK_CONFIG': '/content/.config',\n",
              "        'NV_LIBCUBLAS_PACKAGE': 'libcublas-11-8=11.11.3.6-1',\n",
              "        'NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE': 'cuda-nsight-compute-11-8=11.8.0-1',\n",
              "        'COLAB_RELEASE_TAG': 'release-colab_20231121-060051_RC00',\n",
              "        'KMP_TARGET_PORT': '9000',\n",
              "        'KMP_EXTRA_ARGS': '--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https://colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/m-s-ui1ez2n1wuk3 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true',\n",
              "        'NV_LIBNPP_DEV_PACKAGE': 'libnpp-dev-11-8=11.8.0.86-1',\n",
              "        'COLAB_LANGUAGE_SERVER_PROXY_LSP_DIRS': '/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages',\n",
              "        'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-11-8',\n",
              "        'COLAB_KERNEL_MANAGER_PROXY_PORT': '6000',\n",
              "        'CLOUDSDK_PYTHON': 'python3',\n",
              "        'NV_LIBNPP_DEV_VERSION': '11.8.0.86-1',\n",
              "        'NO_GCE_CHECK': 'False',\n",
              "        'PYTHONPATH': '/env/python',\n",
              "        'NV_LIBCUSPARSE_DEV_VERSION': '11.7.5.86-1',\n",
              "        'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs',\n",
              "        'NV_CUDNN_VERSION': '8.9.6.50',\n",
              "        'SHLVL': '0',\n",
              "        'NV_CUDA_LIB_VERSION': '11.8.0-1',\n",
              "        'COLAB_LANGUAGE_SERVER_PROXY': '/usr/colab/bin/language_service',\n",
              "        'NVARCH': 'x86_64',\n",
              "        'NV_CUDNN_PACKAGE_DEV': 'libcudnn8-dev=8.9.6.50-1+cuda11.8',\n",
              "        'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-11-8',\n",
              "        'NV_LIBNCCL_PACKAGE': 'libnccl2=2.15.5-1+cuda11.8',\n",
              "        'LD_LIBRARY_PATH': '/usr/local/nvidia/lib:/usr/local/nvidia/lib64',\n",
              "        'COLAB_GPU': '',\n",
              "        'NV_CUDA_NSIGHT_COMPUTE_VERSION': '11.8.0-1',\n",
              "        'GCS_READ_CACHE_BLOCK_SIZE_MB': '16',\n",
              "        'NV_NVPROF_VERSION': '11.8.87-1',\n",
              "        'LC_ALL': 'en_US.UTF-8',\n",
              "        'COLAB_FILE_HANDLER_ADDR': 'localhost:3453',\n",
              "        'PATH': '/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin',\n",
              "        'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2',\n",
              "        'COLAB_DEBUG_ADAPTER_MUX_PATH': '/usr/local/bin/dap_multiplexer',\n",
              "        'NV_LIBNCCL_PACKAGE_VERSION': '2.15.5-1',\n",
              "        'PYTHONWARNINGS': 'ignore:::pip._internal.cli.base_command',\n",
              "        'DEBIAN_FRONTEND': 'noninteractive',\n",
              "        'COLAB_BACKEND_VERSION': 'next',\n",
              "        'OLDPWD': '/',\n",
              "        'JPY_PARENT_PID': '76',\n",
              "        'TERM': 'xterm-color',\n",
              "        'CLICOLOR': '1',\n",
              "        'PAGER': 'cat',\n",
              "        'GIT_PAGER': 'cat',\n",
              "        'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',\n",
              "        'ENABLE_DIRECTORYPREFETCHER': '1',\n",
              "        'USE_AUTH_EPHEM': '1',\n",
              "        'PYDEVD_USE_FRAME_EVAL': 'NO',\n",
              "        'OPENAI_API_KEY': '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'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch = FAISS.from_texts(texts, embeddings)"
      ],
      "metadata": {
        "id": "9C8py6wQXE5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_t_EpZ_XGz2",
        "outputId": "b47af9d6-5b8b-4077-f3b9-2894c1935ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain.vectorstores.faiss.FAISS at 0x7a28ec660160>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "wpQ2VnBvXI2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "_L_Ywm-iXLhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"who is data scientist?\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3mtAth2jXNKO",
        "outputId": "f809f7f5-95fe-4011-e04f-b082bcc7a149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Data scientists are people who use their creativity, curiosity, technical skills, and detail-oriented nature to solve problems using data.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "KOirjPyZtIla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what are the best machine learning algorithms for time series analysis, their strengths and weaknesses?\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "id": "qLynnMo0cj8m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "54aa4455-2450-49d9-930d-895e18ef28ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Neural networks are a good choice for evaluating weekly variable contributions, as they can condition upon week without greatly increasing the complexity. Other algorithms such as principal component regression and unsupervised learning techniques may also be beneficial, depending on the goal of the analysis. Each of these algorithms have their own strengths and weaknesses, so it is best to research each before making a decision.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what are the best machine learning algorithms for performance prediction, their strengths and weaknesses?\"\n",
        "docs = docsearch.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6n9NFGs6qEk4",
        "outputId": "dbf77dfa-a98b-412f-c012-1afd7a3b1023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' It depends on the data and the problem, but some common machine learning algorithms for performance prediction include regression, clustering, classification, and recommendation. Each has its own strengths and weaknesses, so it is best to evaluate them based on the specific data and problem.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}